#!/usr/bin/env python

import os
import sys
from threading import Thread, Lock

try:
    from optparse import Option, OptionParser
    from gppylib.gpparseopts import OptParser, OptChecker
    from gppylib.gparray import GpArray
    from gppylib.gphostcache import *
    from gppylib.gplog import *
    from gppylib.commands.unix import *
    from gppylib.commands.gp import *
    from gppylib.db import dbconn
    from gppylib.userinput import *
    from pygresql.pg import DatabaseError
    from gppylib.gpcoverage import GpCoverage
    from pygresql.pgdb import DatabaseError
    from pygresql import pg
    from datetime import datetime

except ImportError, e:
    sys.exit('Cannot import modules.  Please check that you have sourced greenplum_path.sh.  Detail: ' + str(e))

##############
EXECNAME = os.path.split(__file__)[-1]
setup_tool_logging(EXECNAME, getLocalHostname(), getUserName())
logger = get_default_logger()


##############

def print_help(option, opt, value, parser):
    # This function signature is mandatory as this is a callback for OptionParse
    # even if we only use :param parser:
    print("\nUsage: python gpcheckintegrity \n\n" \
          "*****************************************************\n" \
          "SYNOPSIS\n" \
          "*****************************************************\n\n" \
          "{0} -v [--verbose] -s [--schema] <schema_name> -d [--database] <database-name> \n" \
          "{0} -h [-?] [--help] \n\n" \
          "***************************************************** \n" \
          "DESCRIPTION \n" \
          "***************************************************** \n\n" \
          "Utility for Greenplum Database that performs a table integrity exam to verify that all the " \
          "tables in a specific database can be queried. \n\n" \
          "*********************************** \n" \
          "OPTIONS \n" \
          "*********************************** \n\n" \
          "-v, --verbose \t\t\t" \
          "    Enables extra debug output. Only useful for debugging the tool itself. \n" \
          "-s, --schema <schema_name> \t" \
          "    Schema name to fetch the tables from. If this option is set, gpcheckintegrity" \
          " will limit the integrity check to the tables located in this schema \n" \
          "-d, --database <database> \t" \
          "    Name of the database to check. This option is mandatory \n" \
          "-h, -?, --help \t\t\t" \
          "    Prints this help page \n\n" \
          "***************************************************** \n" \
          "EXAMPLES \n" \
          "***************************************************** \n\n" \
          "Check the integrity of all the tables in the database 'sales': \n\n" \
          "\t {0} -d sales \n\n" \
          "Check the integrity of all the tables in 'division_1' from DB 'sales': \n\n" \
          "\t {0} -s division_1 -d sales \n\n" \
          "Display this message again: \n\n" \
          "\t {0} -h \n\n" \
          "***************************************************** \n" \
          "BUGS \n" \
          "***************************************************** \n\n" \
          "Please report any bugs in https://github.com/ielizaga/gpcheckintegrity\n".format(__file__))
    parser.exit()

def parseargs():
    # TODO: use global class to keep some variables present at all times
    parser = OptParser(option_class=OptChecker)
    parser.remove_option('-h')
    parser.add_option('-h', '-?', '--help', action="callback", callback=print_help)
    parser.add_option('-v', '--verbose', action='store_true')
    parser.add_option('-s', '--schema', type='string')
    # TODO: perform check just for one/multiple schemas
    parser.add_option('-A', '--all', action='store_true')
    parser.add_option('-d', '--database', type='string')
    (options, args) = parser.parse_args()

    USER = os.getenv('USER')
    if USER is None or USER is ' ':
        logger.error('USER environment variable must be set')
        parser.exit()

    if options.database is None and options.all is None:
        logger.error('Database must be specified')
        parser.exit()

    if options.database is not None and options.all is not None:
        logger.info('Can\'t specify -A and -d options at the same time')
        parser.exit()

    if options.database is not None:
        logger.info("Checking integrity of database %s" % options.database)

    if options.all is not None:
        logger.info("Checking integrity of all databases")

    return options


def connect(user=None, password=None, host=None, port=None, database=None, utilityMode=False):
    conf = utilityMode and '-c gp_session_role=utility' or None
    if not user:
        user = os.environ.get('PGUSER')
    if not user:
        user = os.environ.get('USER')
    if not password:
        password = os.environ.get('PGPASSWORD')
    if not host:
        host = 'localhost'
    if not port:
        port = int(os.environ.get('PGPORT', 5432))
    if not database:
        database = os.environ.get('PGDATABASE', 'template1')
    try:
        logger.debug('connecting to %s:%s %s' % (host, port, database))
        db = pg.connect(host=host, port=port, user=user,
                        passwd=password, dbname=database, opt=conf)
    except pg.InternalError, ex:
        logger.fatal('could not connect to %s: "%s"' %
                     (database, str(ex).strip()))
        exit(1)

    logger.debug('connected with %s:%s %s' % (host, port, database))
    return db


def get_gp_segment_configuration(database=None):
    cfg = {}
    db = connect(database=database)

    qry = '''
          SELECT content, preferred_role = 'p' as definedprimary,
                 dbid, role = 'p' as isprimary, hostname, address, port,
                 fselocation as datadir
            FROM gp_segment_configuration JOIN pg_filespace_entry on (dbid = fsedbid)
           WHERE fsefsoid = (select oid from pg_filespace where fsname='pg_system')
             AND (role = 'p' or content < 0 )
          '''
    curs = db.query(qry)
    for row in curs.dictresult():
        if row['content'] == -1:
            continue  # skip master
        cfg[row['dbid']] = row
    db.close()
    return cfg


def get_databases():
    """
    This function returns the list of databases present in the Greenplum cluster.
    :return: list of databases
    """
    db = connect(database='template1')
    database_list = []

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT datname
          FROM pg_database
          where datname not like 'template0'
          '''
    curs = db.query(qry)
    for row in curs.dictresult():
        database_list.append(row)
    db.close()
    return database_list


def get_tables(database=None):
    db = connect(database=database)
    table_list = []

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT n.nspname as schema, c.relname as table
            FROM pg_class c join pg_namespace n ON (c.relnamespace = n.oid)
           WHERE (c.relkind = 'r') and (relstorage != 'x') and (n.nspname != 'pg_catalog') and (c.relhassubclass='f')
          '''
    curs = db.query(qry)
    for row in curs.dictresult():
        table_list.append(row)
    db.close()
    return table_list


def get_tables_in_schema(database, schema):
    """
    This function returns a list of tables within :param schema:. If the schema is pg_catalog, it returns None and
    logs an error.
    :param database: name of the database
    :param schema: schema to fetch table names from
    :return: list of tables or None if :param schema: is 'pg_catalog'
    :raise ValueError: if schema is 'pg_catalog'
    """
    db = connect(database=database)
    table_list = []

    if schema == "pg_catalog":
        logger.error("This check is ineffective on catalog tables. Use gpcheckcat to check catalog integrity")
        raise ValueError("Schema is pg_catalog")

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT n.nspname as schema, c.relname as table
            FROM pg_class c join pg_namespace n ON (c.relnamespace = n.oid)
           WHERE (c.relkind = 'r') and (relstorage != 'x') and (n.nspname = '%s') and (c.relhassubclass='f')
          ''' % pg.escape_string(schema)
    curs = db.query(qry)

    for row in curs.dictresult():
        table_list.append(row)

    db.close()

    return table_list


def spawn_threads(database, schema=None):
    dbids = get_gp_segment_configuration()  # get Greenplum segment information
    tables = list()

    if schema is not None:
        tables = get_tables_in_schema(database, schema)
        logger.info("Checking only tables in schema %s" % schema)
    else:
        tables = get_tables(database=database)  # get table list

    threads = []

    for dbid in dbids:
        if dbids[dbid]['isprimary'] == 't':
            th = CheckIntegrity(tables, dbids[dbid]['hostname'], database, dbids[dbid]['content'], dbids[dbid]['port'])
            th.start()
            threads.append(th)

    for thread in threads:
        logger.debug('waiting on thread %s' % thread.getName())
        thread.join()


class CheckIntegrity(Thread):
    def __init__(self, tables, hostname, database, content, port):
        Thread.__init__(self)
        self.hostname = hostname
        self.database = database
        self.port = port
        self.content = content
        self.tables = tables

    def run(self):
        db = connect(database=self.database, host=self.hostname, port=self.port, utilityMode=True)
        for table in self.tables:
            try:
                logger.info("[seg%s] {%s} Checking table %s.%s" % (
                    self.content, self.database, table['schema'], table['table']))
                qry = '''
                      COPY %s.%s
                      TO '/dev/null'
                      ''' % (table['schema'], table['table'])
                db.query(qry)
            except Exception, de:
                # TODO: better error summary report
                logger.error('[seg%s] {%s} Failed for table %s.%s' % (
                    self.content, self.database, table['schema'], table['table']))
                logger.debug('[seg%s] {%s} %s' % (self.content, self.database, str(de).strip()))

                # Append this table name to reported_table list
                table_lock.acquire()
                reported_tables.append(
                    "[seg%s] {%s} %s.%s in %s:%d" % (self.content, self.database, table['schema'], table['table'],
                                                     self.hostname, self.port))
                table_lock.release()

                # TODO: pygresql wraps all queries in the same cursor under the same transaction
                db.close()
                db = connect(database=self.database, host=self.hostname, port=self.port, utilityMode=True)
        db.close()


#############
if __name__ == '__main__':
    logger = get_default_logger()
    setup_tool_logging(EXECNAME, getLocalHostname(), getUserName())

    options = parseargs()

    if options.verbose:
        enable_verbose_logging()

    try:
        reported_tables = []
        table_lock = Lock()

        if options.all is not None:
            for db in get_databases():
                # TODO: List number of databases/schemas/tables to be checked and prompt to continue
                logger.info("Checking database %s" % db['datname'])
                spawn_threads(db['datname'])
        else:
            if options.schema is not None:
                spawn_threads(options.database, options.schema)
            else:
                spawn_threads(options.database)

        logger.info("ERROR REPORT SUMMARY %s" % datetime.now())
        logger.info("============================================")

        if len(reported_tables) == 0:
            logger.info("No tables reported inconsistency errors")
        else:
            for t in reported_tables:
                logger.info(t)

    # TODO: Better exception handling
    except Exception, e:
        logger.error('errors in job:')
        logger.error(e.__str__())
        logger.error('exiting early')

    logger.info("completed successfully")
