#!/usr/bin/env python

import os
import string
import sys
import optparse
from threading import Thread, Lock, BoundedSemaphore

try:
    from gppylib.gparray import GpArray
    from gppylib.gphostcache import *
    from gppylib.gplog import *
    from gppylib.commands.unix import *
    from gppylib.commands.gp import *
    from gppylib.db import dbconn
    from gppylib.userinput import *
    from pygresql.pg import DatabaseError
    from gppylib.gpcoverage import GpCoverage
    from pygresql.pgdb import DatabaseError
    from pygresql import pg
    from datetime import datetime

except ImportError, e:
    sys.exit('Cannot import modules.  Please check that you have sourced greenplum_path.sh.  Detail: ' + str(e))

##############
EXECNAME = os.path.split(__file__)[-1]
VERSION = '1.0'
DEFAULT_NUM_THREADS = 32

##############


def parseargs():
    parser = optparse.OptionParser(prog='gpcheckintegrity', usage="%prog [options] -d database | -A ", version=VERSION,
                                   description='Utility for Greenplum Database that performs a table integrity exam to'
                                               ' verify that all the tables in a specific database can be accessed.',
                                   epilog='Source code available at github.com/ielizaga/gpcheckintegrity')
    parser.add_option('-v', '--verbose', action='store_true', help='Enables debug logging')
    parser.add_option('-A', '--all', action='store_true', help='Check all databases')
    parser.add_option('-d', '--database', help='Database to connect to')
    parser.add_option('-s', '--schema', help='Checks all the tables in this schema')
    parser.add_option('-S', '--schema-list', dest='schema_list',
                      help='Comma-separated list of schemas to check tables from. Example: s1,s2,s3')
    parser.add_option('-t', '--table', help='Check just this table')
    parser.add_option('-B', '--parallel', type=int, default=DEFAULT_NUM_THREADS, help='Number of parallel workers (Default: 32)')
    (options_object, args_object) = parser.parse_args()

    USER = os.getenv('USER')
    if USER is None or USER is ' ':
        logger.error('USER environment variable must be set')
        parser.exit()

    if options_object.database is None and options_object.all is None:
        logger.error('Database must be specified')
        parser.exit()

    if options_object.database is not None and options_object.all is not None:
        logger.info('Can\'t specify -A and -d options at the same time')
        parser.exit()

    if options_object.database is not None:
        logger.info("Checking integrity of database %s" % options_object.database)

    if options_object.all is not None:
        logger.info("Checking integrity of all databases")

    return options_object


def connect(user=None, password=None, host=None, port=None, database=None, utilityMode=False):
    conf = utilityMode and '-c gp_session_role=utility' or None
    if not user:
        user = os.environ.get('PGUSER')
    if not user:
        user = os.environ.get('USER')
    if not password:
        password = os.environ.get('PGPASSWORD')
    if not host:
        host = 'localhost'
    if not port:
        port = int(os.environ.get('PGPORT', 5432))
    if not database:
        database = os.environ.get('PGDATABASE', 'template1')
    try:
        logger.debug('connecting to %s:%s %s' % (host, port, database))
        db = pg.connect(host=host, port=port, user=user,
                        passwd=password, dbname=database, opt=conf)
    except pg.InternalError, ex:
        logger.fatal('could not connect to %s: "%s"' %
                     (database, str(ex).strip()))
        exit(1)

    logger.debug('connected with %s:%s %s' % (host, port, database))
    return db


def get_gp_segment_configuration(database=None):
    cfg = {}
    db = connect(database=database)

    qry = '''
          SELECT content, preferred_role = 'p' as definedprimary,
                 dbid, role = 'p' as isprimary, hostname, address, port,
                 fselocation as datadir
            FROM gp_segment_configuration JOIN pg_filespace_entry on (dbid = fsedbid)
           WHERE fsefsoid = (select oid from pg_filespace where fsname='pg_system')
             AND (role = 'p' or content < 0 )
          '''
    curs = db.query(qry)
    for row in curs.dictresult():
        if row['content'] == -1:
            continue  # skip master
        cfg[row['dbid']] = row
    db.close()
    return cfg


def get_databases():
    """
    This function returns the list of databases present in the Greenplum cluster.
    :return: list of databases
    """
    db = connect(database='template1')
    database_list = []

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT datname
          FROM pg_database
          where datname not like 'template0'
          '''
    curs = db.query(qry)
    for row in curs.dictresult():
        database_list.append(row)
    db.close()
    return database_list


def get_tables(database=None):
    db = connect(database=database)
    table_list = []

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT n.nspname as schema, c.relname as table
            FROM pg_class c join pg_namespace n ON (c.relnamespace = n.oid)
           WHERE (c.relkind = 'r') and (relstorage != 'x') and (n.nspname != 'pg_catalog') and (c.relhassubclass='f')
          '''
    curs = db.query(qry)
    for row in curs.dictresult():
        table_list.append(row)
    db.close()
    return table_list


def get_table(database, table_str):
    logger.debug("Parsing table string: %s" % table_str)
    table_cmdline_split = string.split(table_str, '.')

    if len(table_cmdline_split) == 1:
        schema_name = 'public'
        table_name = table_cmdline_split[0]
    elif len(table_cmdline_split) == 2:
        schema_name = table_cmdline_split[0]
        table_name = table_cmdline_split[1]
    else:
        logger.debug("Processing table string returned more than 2 fields")
        return None

    db_conn = connect(database=database)
    qry = '''
        SELECT n.nspname as schema, c.relname as table
          FROM pg_class c join pg_namespace n ON (c.relnamespace = n.oid)
        WHERE (c.relkind = 'r') and (relstorage != 'x') and (n.nspname != 'pg_catalog')
        and (c.relname = '%(table_name)s') and (n.nspname = '%(schema_name)s')
    ''' % {"table_name": pg.escape_string(table_name), "schema_name": pg.escape_string(schema_name)}

    curs = db_conn.query(qry)

    if curs.ntuples() == 0:
        db_conn.close()
        logger.warn("Table %s does not exists" % table_str)
        return None
    else:
        table_dict = curs.dictresult()
        db_conn.close()
        return table_dict


def get_tables_in_schema(database, schema):
    """
    This function returns a list of tables within :param schema:. If the schema is pg_catalog, it returns None and
    logs an error.
    :param database: name of the database
    :param schema: schema to fetch table names from
    :return: list of tables or None if :param schema: is 'pg_catalog'
    :raise ValueError: if schema is 'pg_catalog'
    """
    db = connect(database=database)
    table_list = []

    if schema == "pg_catalog":
        logger.error("This check is ineffective on catalog tables. Use gpcheckcat to check catalog integrity")
        raise ValueError("Schema is pg_catalog")

    # Retrieve all non-catalog/non partitioned parent tables
    qry = '''
          SELECT n.nspname as schema, c.relname as table
            FROM pg_class c join pg_namespace n ON (c.relnamespace = n.oid)
           WHERE (c.relkind = 'r') and (relstorage != 'x') and (n.nspname = '%s') and (c.relhassubclass='f')
          ''' % pg.escape_string(schema)
    curs = db.query(qry)

    for row in curs.dictresult():
        table_list.append(row)

    db.close()

    return table_list


def database_schema_exists(database, schema):

    db = connect(database=database)
    qry = '''
    select True as exists from pg_namespace a where a.nspname = '%s'
    ''' % pg.escape_string(schema)

    logger.debug("Running query: %s" % qry)

    curs = db.query(qry)

    return curs.ntuples() == 1


def _spawn_threads(database):

    tables = list()

    if options.schema_list is not None:
        logger.debug("Parsing schema list: %s" % options.schema_list)
        schemas = string.split(options.schema_list, ',')
        for s in schemas:
            if database_schema_exists(database, s):
                logger.debug("Adding schema %s to checklist" % s)
                tables.extend(get_tables_in_schema(database, s))
            else:
                logger.warn("Schema %s not found" % s)

    if options.schema is not None:
        tables.extend(get_tables_in_schema(database, options.schema))
        logger.info("Checking integrity of schema %s" % options.schema)

    if options.table is not None:
        table_dict = get_table(options.database, options.table)

        if table_dict is not None:
            tables.extend(table_dict)

    if options.table is None and options.schema is None and options.schema_list is None:
        tables.extend(get_tables(database))

    dbids = get_gp_segment_configuration()  # get Greenplum segment information
    threads = []
    for dbid in dbids:
        if dbids[dbid]['isprimary'] == 't':
            th = CheckIntegrity(tables, dbids[dbid]['hostname'], database, dbids[dbid]['content'],
                                dbids[dbid]['port'])
            th.start()
            threads.append(th)

    for thread in threads:
        logger.debug('waiting on thread %s' % thread.getName())
        thread.join()

    return


class CheckIntegrity(Thread):
    def __init__(self, tables, hostname, database, content, port):
        Thread.__init__(self)
        self.hostname = hostname
        self.database = database
        self.port = port
        self.content = content
        self.tables = tables

    def run(self):
        pool_semaphore.acquire()
        db_conn = connect(database=self.database, host=self.hostname, port=self.port, utilityMode=True)
        for table in self.tables:
            try:
                logger.info("[seg%s] {%s} Checking table %s.%s" % (
                    self.content, self.database, table['schema'], table['table']))
                qry = '''
                      COPY "%s"."%s"
                      TO '/dev/null'
                      ''' % (table['schema'], table['table'])
                db_conn.query(qry)
            except DatabaseError, de:
                # TODO: better error summary report
                logger.error('[seg%s] {%s} Failed for table %s.%s' % (
                    self.content, self.database, table['schema'], table['table']))
                logger.debug('[seg%s] {%s} %s' % (self.content, self.database, str(de).strip()))

                # Append this table name to reported_table list
                table_lock.acquire()
                reported_tables.append(
                    "[seg%s] {%s} %s.%s in %s:%d" % (self.content, self.database, table['schema'], table['table'],
                                                     self.hostname, self.port))
                table_lock.release()

                # TODO: pygresql wraps all queries in the same cursor under the same transaction
                db_conn.close()
                db_conn = connect(database=self.database, host=self.hostname, port=self.port, utilityMode=True)
            except Exception, unexpected_exception:
                logger.error("[seg%s] {%s} Failed with unexpected exception: " % (self.content, self.database))
                logger.error(unexpected_exception)
                break

        try:
            db_conn.close()
        except pg.InternalError, pgie:
            logger.debug(
                'Attempted to close an already closed connection. This probably means that a thread hit '
                'an unexpected error')
            logger.debug(pgie)

        pool_semaphore.release()


#############
if __name__ == '__main__':
    logger = get_default_logger()
    setup_tool_logging(EXECNAME, getLocalHostname(), getUserName())

    options = parseargs()

    if options.verbose:
        enable_verbose_logging()

    try:
        reported_tables = []
        table_lock = Lock()

        logger.debug("Maximum number of threads is %d" % options.parallel)
        pool_semaphore = BoundedSemaphore(options.parallel)

        if options.all is not None:
            for db in get_databases():
                # TODO: List number of databases/schemas/tables to be checked and prompt to continue
                logger.info("Checking database %s" % db['datname'])
                _spawn_threads(db['datname'])
        else:
                _spawn_threads(options.database)

        logger.info("ERROR REPORT SUMMARY %s" % datetime.now())
        logger.info("============================================")

        if len(reported_tables) == 0:
            logger.info("No tables reported inconsistency errors")
        else:
            for t in reported_tables:
                logger.info(t)

    # TODO: Better exception handling
    except Exception, e:
        logger.error('errors in job:')
        logger.error(e.__str__())
        logger.error('exiting early')

    logger.info("completed successfully")
